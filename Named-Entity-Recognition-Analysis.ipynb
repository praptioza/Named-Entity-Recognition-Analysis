{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the world of Named Entity Recognition (NER) with Python! This project dives into four popular libraries - NLTK, spaCy, Huggingface/Transformers, and stanza - to extract named entities from text data. By evaluating their performance on a set of 20 test cases, we gain insights into their accuracy, precision, and recall using micro-averaging. Spoiler alert: Huggingface/Transformers' ner pipeline steals the show with its top-notch performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data =  [[('John', 'PERSON'), ('works', 'O'), ('at', 'O'), ('Apple', 'ORGANIZATION')], [('Alice', 'PERSON'), ('studies', 'O'), ('at', 'O'), ('Stanford', 'ORGANIZATION'), ('in', 'O'), ('California', 'LOCATION')], [('The', 'O'), ('CEO', 'TITLE'), ('of', 'O'), ('Microsoft', 'ORGANIZATION'), ('is', 'O'), ('speaking', 'O'), ('today', 'O')], [('Google', 'ORGANIZATION'), ('announced', 'O'), ('a', 'O'), ('new', 'O'), ('smartphone', 'O')], [('Facebook', 'ORGANIZATION'), ('acquired', 'O'), ('Instagram', 'ORGANIZATION')], [('The', 'O'), ('movie', 'O'), ('premieres', 'O'), ('in', 'O'), ('New', 'LOCATION'), ('York', 'LOCATION'), ('City', 'LOCATION')], [('Leonardo', 'PERSON'), ('Da', 'PERSON'), ('Vinci', 'PERSON'), ('painted', 'O'), ('the', 'O'), ('Mona', 'O'), ('Lisa', 'O')], [('The', 'O'), ('Nobel', 'O'), ('Prize', 'O'), ('was', 'O'), ('awarded', 'O'), ('to', 'O'), ('three', 'O'), ('scientists', 'O')], [('Barack', 'PERSON'), ('Obama', 'PERSON'), ('was', 'O'), ('the', 'O'), ('president', 'TITLE'), ('of', 'O'), ('the', 'O'), ('United', 'LOCATION'), ('States', 'LOCATION')], [('The', 'O'), ('Amazon', 'ORGANIZATION'), ('Rainforest', 'LOCATION'), ('is', 'O'), ('vital', 'O'), ('for', 'O'), ('the', 'O'), (\"planet's\", 'O'), ('climate', 'O')], [('Marie', 'PERSON'), ('Curie', 'PERSON'), ('won', 'O'), ('two', 'O'), ('Nobel', 'O'), ('Prizes', 'O')], [('Shakespeare', 'PERSON'), ('wrote', 'O'), ('Romeo', 'O'), ('and', 'O'), ('Juliet', 'O')], [('Angela', 'PERSON'), ('Merkel', 'PERSON'), ('was', 'O'), ('the', 'O'), ('Chancellor', 'TITLE'), ('of', 'O'), ('Germany', 'LOCATION')], [('The', 'O'), ('Eiffel', 'LOCATION'), ('Tower', 'LOCATION'), ('is', 'O'), ('in', 'O'), ('Paris', 'LOCATION')], [('Amazon', 'ORGANIZATION'), ('was', 'O'), ('founded', 'O'), ('by', 'O'), ('Jeff', 'PERSON'), ('Bezos', 'PERSON')], [('Tesla', 'ORGANIZATION'), ('is', 'O'), ('expanding', 'O'), ('its', 'O'), ('market', 'O'), ('in', 'O'), ('Europe', 'LOCATION')], [('Harvard', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('is', 'O'), ('located', 'O'), ('in', 'O'), ('Massachusetts', 'LOCATION')], [('The', 'O'), ('Mona', 'O'), ('Lisa', 'O'), ('is', 'O'), ('displayed', 'O'), ('in', 'O'), ('the', 'O'), ('Louvre', 'LOCATION')], [('The', 'O'), ('Sphinx', 'LOCATION'), ('and', 'O'), ('the', 'O'), ('Pyramids', 'LOCATION'), ('are', 'O'), ('in', 'O'), ('Egypt', 'LOCATION')], [('Leonardo', 'PERSON'), ('DiCaprio', 'PERSON'), ('won', 'O'), ('an', 'O'), ('Oscar', 'O'), ('for', 'O'), ('The', 'O'), ('Revenant', 'O')]]\n",
      "Testing data = [['Bill', 'Gates', 'founded', 'Microsoft'], ['The', 'Louvre', 'Museum', 'is', 'in', 'Paris'], ['Mount', 'Fuji', 'is', 'a', 'famous', 'landmark', 'in', 'Japan'], ['The', 'United', 'Nations', 'was', 'formed', 'in', '1945'], ['Shakira', 'performed', 'at', 'the', 'Super', 'Bowl', 'halftime', 'show'], ['The', 'Nobel', 'Peace', 'Prize', 'was', 'awarded', 'to', 'Malala', 'Yousafzai'], ['The', 'Amazon', 'River', 'flows', 'through', 'Brazil'], ['The', 'Pyramids', 'of', 'Giza', 'are', 'in', 'Egypt'], ['Rome', 'is', 'the', 'capital', 'of', 'Italy'], ['The', 'Great', 'Wall', 'of', 'China', 'is', 'one', 'of', 'the', 'Seven', 'Wonders', 'of', 'the', 'World']]\n"
     ]
    }
   ],
   "source": [
    "# Read the content of the text file\n",
    "with open('A3-3-Data.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Split the content into training_data and test_data sections\n",
    "training_start = content.find(\"training_data = [\")\n",
    "test_start = content.find(\"test_data = [\", training_start)\n",
    "training_end = test_start\n",
    "\n",
    "# Extract training_data\n",
    "training_data_content = content[training_start:training_end]\n",
    "exec(training_data_content)  # Execute the extracted content to load the training_data\n",
    "\n",
    "# Extract test_data\n",
    "test_data_content = content[test_start:]\n",
    "exec(test_data_content)  # Execute the extracted content to load the test_data\n",
    "\n",
    "training_data = training_data\n",
    "test_data = test_data\n",
    "\n",
    "# Print or use data1 and data2 as needed\n",
    "print(\"Training data = \", training_data)\n",
    "print(\"Testing data =\", test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk import ne_chunk\n",
    "import spacy\n",
    "import stanza\n",
    "from transformers import pipeline, BertConfig   \n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "from nltk.tree import Tree\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddf26f999074ac3a42b6c286f2f7de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 07:40:43 INFO: Downloaded file to C:\\Users\\prapt\\stanza_resources\\resources.json\n",
      "2024-06-03 07:40:43 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-06-03 07:40:46 INFO: File exists: C:\\Users\\prapt\\stanza_resources\\en\\default.zip\n",
      "2024-06-03 07:40:52 INFO: Finished downloading models and saved to C:\\Users\\prapt\\stanza_resources\n",
      "2024-06-03 07:40:52 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fd54f8cab042f6ba571005c398715d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 07:40:52 INFO: Downloaded file to C:\\Users\\prapt\\stanza_resources\\resources.json\n",
      "2024-06-03 07:40:52 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-06-03 07:40:53 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-06-03 07:40:53 INFO: Using device: cpu\n",
      "2024-06-03 07:40:53 INFO: Loading: tokenize\n",
      "2024-06-03 07:40:53 INFO: Loading: mwt\n",
      "2024-06-03 07:40:53 INFO: Loading: ner\n",
      "2024-06-03 07:40:54 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the stanza pipeline for English\n",
    "stanza.download('en')  # download English model\n",
    "nlp_stanza = stanza.Pipeline('en', processors='tokenize,ner')\n",
    "\n",
    "# Load the English language model\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **STEP 1:**\n",
    "##### Code from scratch to implement the Viterbi algorithm and a Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitions: \n",
      "Transition probabilities from 'PERSON':\n",
      "  'O': 8\n",
      "  'PERSON': 7\n",
      "\n",
      "Transition probabilities from 'O':\n",
      "  'O': 56\n",
      "  'ORGANIZATION': 5\n",
      "  'LOCATION': 12\n",
      "  'TITLE': 3\n",
      "  'PERSON': 1\n",
      "\n",
      "Transition probabilities from 'ORGANIZATION':\n",
      "  'O': 7\n",
      "  'LOCATION': 1\n",
      "  'ORGANIZATION': 1\n",
      "\n",
      "Transition probabilities from 'TITLE':\n",
      "  'O': 3\n",
      "\n",
      "Transition probabilities from 'LOCATION':\n",
      "  'LOCATION': 4\n",
      "  'O': 4\n",
      "\n",
      "[['PERSON', 'PERSON', 'PERSON', 'PERSON'], ['O', 'LOCATION', 'LOCATION', 'O', 'O', 'TITLE'], ['LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'O', 'TITLE'], ['O', 'LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'O', 'TITLE'], ['LOCATION', 'LOCATION', 'O', 'O', 'TITLE', 'TITLE', 'TITLE', 'TITLE'], ['O', 'PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERSON'], ['O', 'TITLE', 'TITLE', 'TITLE', 'TITLE', 'TITLE'], ['O', 'O', 'O', 'O', 'O', 'O', 'TITLE'], ['O', 'O', 'O', 'TITLE', 'TITLE', 'TITLE'], ['O', 'LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'O', 'O', 'O', 'O', 'LOCATION', 'LOCATION', 'O', 'O', 'TITLE']]\n",
      "Sentence 1:\n",
      "Words: ['Bill', 'Gates', 'founded', 'Microsoft']\n",
      "Predicted Tags: ['PERSON', 'PERSON', 'PERSON', 'PERSON']\n",
      "\n",
      "Sentence 2:\n",
      "Words: ['The', 'Louvre', 'Museum', 'is', 'in', 'Paris']\n",
      "Predicted Tags: ['O', 'LOCATION', 'LOCATION', 'O', 'O', 'TITLE']\n",
      "\n",
      "Sentence 3:\n",
      "Words: ['Mount', 'Fuji', 'is', 'a', 'famous', 'landmark', 'in', 'Japan']\n",
      "Predicted Tags: ['LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'O', 'TITLE']\n",
      "\n",
      "Sentence 4:\n",
      "Words: ['The', 'United', 'Nations', 'was', 'formed', 'in', '1945']\n",
      "Predicted Tags: ['O', 'LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'O', 'TITLE']\n",
      "\n",
      "Sentence 5:\n",
      "Words: ['Shakira', 'performed', 'at', 'the', 'Super', 'Bowl', 'halftime', 'show']\n",
      "Predicted Tags: ['LOCATION', 'LOCATION', 'O', 'O', 'TITLE', 'TITLE', 'TITLE', 'TITLE']\n",
      "\n",
      "Sentence 6:\n",
      "Words: ['The', 'Nobel', 'Peace', 'Prize', 'was', 'awarded', 'to', 'Malala', 'Yousafzai']\n",
      "Predicted Tags: ['O', 'PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERSON']\n",
      "\n",
      "Sentence 7:\n",
      "Words: ['The', 'Amazon', 'River', 'flows', 'through', 'Brazil']\n",
      "Predicted Tags: ['O', 'TITLE', 'TITLE', 'TITLE', 'TITLE', 'TITLE']\n",
      "\n",
      "Sentence 8:\n",
      "Words: ['The', 'Pyramids', 'of', 'Giza', 'are', 'in', 'Egypt']\n",
      "Predicted Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'TITLE']\n",
      "\n",
      "Sentence 9:\n",
      "Words: ['Rome', 'is', 'the', 'capital', 'of', 'Italy']\n",
      "Predicted Tags: ['O', 'O', 'O', 'TITLE', 'TITLE', 'TITLE']\n",
      "\n",
      "Sentence 10:\n",
      "Words: ['The', 'Great', 'Wall', 'of', 'China', 'is', 'one', 'of', 'the', 'Seven', 'Wonders', 'of', 'the', 'World']\n",
      "Predicted Tags: ['O', 'LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'O', 'O', 'O', 'O', 'LOCATION', 'LOCATION', 'O', 'O', 'TITLE']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "class MY_HMM:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.transition_probs = {}\n",
    "        self.emission_probs = {}\n",
    "        self.tag_list = set()\n",
    "\n",
    "    def HMM(self):\n",
    "        transition_probs = {} # nested dict to store transition prob between tags, the dict has the tags and the count of its following tags\n",
    "        emission_probs = {} # nested dict to store the emission probabilities( word-tag pairs), the dict has the tags and the words that are used with those tags\n",
    "        tag_list = set()  # set to collect the unique tags from the given data\n",
    "        \n",
    "        # initialize and add to the transition_probs dictionary, the count of the tranisitions between tags(current_tag to next_tag) based on the training_data\n",
    "        for sentence in training_data:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                current_tag, next_tag = sentence[i][1], sentence[i + 1][1] # extracts the current and next tags from each pair in a sentence\n",
    "                if current_tag not in transition_probs:\n",
    "                    transition_probs[current_tag] = {} # initialize the transition_probs if current tag is the first tag and not in dictionary\n",
    "                if next_tag not in transition_probs[current_tag]:  # if next tag is not already a key, that is no transition from current tag to next tag\n",
    "                    transition_probs[current_tag][next_tag] = 0 # then count it as zero \n",
    "                transition_probs[current_tag][next_tag] += 1 # increment the count of transition from current tag to next \n",
    "        # So tranision counts for first sentence are added from 'PERSON' TO 'O' : transition_probs['PERSON']['O'] = 1, from 'O' to 'O' : transition_probs['O']['O'] = 1, from 'O' to 'ORGANIZATION' : transition_probs['O']['ORGANIZATION'] = 1\n",
    "\n",
    "        # initialize and add the emission counts to the emission_probs, that is the number of occurrences of words of a particular tag\n",
    "        #iterate over each sentence in training data and iterate over each word,tag pair\n",
    "        for sentence in training_data:\n",
    "            for word, tag in sentence:\n",
    "                #if tag is found for the first time and is not in the dictionary then add it and initailize an emoty dictionary\n",
    "                if tag not in emission_probs:\n",
    "                    emission_probs[tag] = {}\n",
    "                # if a word is not in a particular tag in the dictionary then add it and initializa as 0, increment by 1 whenever it occurs \n",
    "                if word not in emission_probs[tag]:\n",
    "                    emission_probs[tag][word] = 0\n",
    "                emission_probs[tag][word] += 1\n",
    "\n",
    "        # get unique tags \n",
    "        for sent in sentence_tags:\n",
    "            tags = sent.split()\n",
    "            tag_list.update(tags)\n",
    "\n",
    "        return list(tag_list), transition_probs, emission_probs\n",
    "    \n",
    "\n",
    "# computes the probability of transitioning from the current tag to the next tag given the transition probabilities and the total number of unique tags\n",
    "def calculate_transition_probability(current_tag, next_tag, transition_probs, total_tags):\n",
    "    # handle both existing and non existing(unseen) transitions\n",
    "    # Check if the current tag has any transitions in the transition_probs dictionary\n",
    "    if current_tag in transition_probs:\n",
    "        # Check if the transition to the next tag is there in the transition_probs dictionary\n",
    "        if next_tag in transition_probs[current_tag]:\n",
    "            # if there is a tranision retrieve it\n",
    "            count_transition = transition_probs[current_tag][next_tag]\n",
    "            # Calculate the transition probability using the Laplace smoothing to handle unseen transitions where there is 0\n",
    "            probability = (count_transition + 1) / (sum(transition_probs[current_tag].values()) + total_tags)\n",
    "        # if tranision to the next tag is not there in the transition_probs dictionary set a default probabiltiy\n",
    "        else:\n",
    "            # Assign a default probability for unseen transitions\n",
    "            probability = 1 / (sum(transition_probs[current_tag].values()) + total_tags)\n",
    "    else:\n",
    "        # Assign a default probability if the current tag has no transitions in the transition_probs dictionary\n",
    "        probability = 1 / total_tags\n",
    "    return probability\n",
    "\n",
    "# Method to calculate and print the transition probability\n",
    "def check_transition_probability(current_tag, next_tag, transition_probs,tag_list ):\n",
    "    if current_tag in transition_probs and next_tag in transition_probs[current_tag]:\n",
    "        count_transition = transition_probs[current_tag][next_tag]\n",
    "        total_count_current_tag = sum(transition_probs[current_tag].values())\n",
    "        probability = (count_transition + 1) / (total_count_current_tag + len(tag_list))\n",
    "        print(f\"Probability of transition from '{current_tag}' to '{next_tag}': {probability:.4f}\")\n",
    "    else:\n",
    "        print(f\"Transition from '{current_tag}' to '{next_tag}' not found.\")\n",
    "\n",
    "# implement the Viterbi algo for sequence tagging using a HMM \n",
    "\n",
    "# implement the Viterbi algo for sequence tagging using a HMM \n",
    "def viterbi():\n",
    "    tag_list, transition_probs, emission_probs = mm.HMM()\n",
    "    # Loop over each key (current tag) in trans_probs\n",
    "    print(\"Transitions: \")\n",
    "    for curr_tag, next_tags in transition_probs.items():\n",
    "        print(f\"Transition probabilities from '{curr_tag}':\")\n",
    "\n",
    "        # Loop over each next_tag and its count in the current tag's dictionary\n",
    "        for next_tag, count in next_tags.items():\n",
    "            print(f\"  '{next_tag}': {count}\")\n",
    "\n",
    "        print()  \n",
    "    \n",
    "    # # Function to print the emission probabilities dictionary in a formatted way\n",
    "    # for tag, words_dict in emission_probs.items():\n",
    "    #     print(f\"Tag: {tag}\")\n",
    "    #     for word, count in words_dict.items():\n",
    "    #         print(f\"  '{word}': {count}\")\n",
    "    #     print()  # Add a blank line for readability\n",
    "\n",
    "    number_of_tag = len(emission_probs) # total number of tags in the training data\n",
    "    predicted_tags = [] # list to store the predicted tag sequences for each of the input sentences from the test data\n",
    " \n",
    "    # iterate over each sentence in the test data\n",
    "    for sentence in test_data:\n",
    "        # create a list of words in each sentence and convert into lowercase\n",
    "        words = [word.lower() for word in sentence]\n",
    "\n",
    "        # Initialize the matrix of all zeroes of the dimensions of total tags and the words in the sentence\n",
    "        viterbi_matrix = np.zeros((number_of_tag, len(words)))\n",
    "\n",
    "        # Initialize backpointer matrix to all zeroes with the dimensions same as matrix to store the backpointers for the best previous state\n",
    "        backpointers = np.zeros((number_of_tag, len(words)), dtype=int)\n",
    "\n",
    "        # iterate over the words from the word list made from the current input sentence\n",
    "        for w in range(len(words)):\n",
    "            # iterate through each tag from the unique tags list\n",
    "            for i in range(len(tag_list)):\n",
    "                # this test_emission_prob is set to zero initially and will to used to store the emission probability of the current word given the current tag for the words of the input sentence of test data \n",
    "                test_emission_prob = 0\n",
    "                # array test_transition_probs initialized to zeroes of length equal to tag list to store the transition probabilties from the current tag to all other tags in the list\n",
    "                test_transition_probs = np.zeros(len(tag_list))\n",
    "\n",
    "                # retrieves the word at current index w fromt he words lsit of the current input sentence\n",
    "                current_token = words[w]\n",
    "                # retrieve the tag at the current index i from the tag list to represent the tag being considered for the w in the input sentence\n",
    "                current_tag = tag_list[i]\n",
    "\n",
    "                # Calculate emission probability for each of the tag(current_tag) given the current_token in the input sentence\n",
    "                # check if the current token is in the emission_probs\n",
    "                if current_token in emission_probs[current_tag]:\n",
    "                    # calculate the emission probability using the laplace smooting for unseen cases\n",
    "                    test_emission_prob = (emission_probs[current_tag][current_token] + 1) / (sum(emission_probs[current_tag].values()) + number_of_tag)\n",
    "                else:\n",
    "                    # else if the current token is not in the emission probs then default probability is assigned\n",
    "                    test_emission_prob = 1 / (sum(emission_probs[current_tag].values()) + number_of_tag)\n",
    "\n",
    "                # execute viterbi algo\n",
    "                if w == 0:  # For the first word, use the initial probability for tranistions\n",
    "                    # initializr all zeroes array to store the transtion probabilities from the current tag to each possible next tags\n",
    "                    test_transition_probs = np.zeros(len(tag_list)) \n",
    "                    for j in range(len(tag_list)):\n",
    "                        next_tag = tag_list[j]\n",
    "                        test_transition_probs[j] = calculate_transition_probability(current_tag, next_tag, transition_probs, number_of_tag)\n",
    "\n",
    "                    # compute initial matrix values for the first word as the product of the transition probabilities and emission probabiltiies\n",
    "                    viterbi_matrix[i][w] = test_transition_probs[i] * test_emission_prob\n",
    "\n",
    "                # for the rest of the words in the input sentence calculate the transition probabilities\n",
    "                else:\n",
    "                    for j in range(len(tag_list)):\n",
    "                        next_tag = tag_list[j]\n",
    "                        test_transition_probs[j] = calculate_transition_probability(current_tag, next_tag, transition_probs, number_of_tag)\n",
    "\n",
    "                    # Multiply previous colummn of probabilities in the viterbi matrix by transition probabilities \n",
    "                    probabilities = viterbi_matrix[:, w - 1] * test_transition_probs\n",
    "                    # the index of the highest probability in this probabilities  is stored as backpointer inthe backpointers matrix to do backtracking to find the best path\n",
    "                    backpointers[i][w] = np.argmax(probabilities)\n",
    "                    # change the viterbi matrix position for the current tag and word to the maximum probabiltiy found multiplied by the emission probability\n",
    "                    viterbi_matrix[i][w] = np.max(probabilities) * test_emission_prob\n",
    "\n",
    "        # backtrack to retrieve the best tag sequence that is the best_path and so initialize a list to store this best path for the current input sentence\n",
    "        best_path = []\n",
    "        # holds the maximum probability found in the last column of the viterbi matrix that is the highest probability of any tag sequence ending at the last word of the sentence\n",
    "        best_path_prob = np.max(viterbi_matrix[:, -1])\n",
    "        # find the best final state by using the tag index with the maximum viterbi score in the last column\n",
    "        best_final_state = np.argmax(viterbi_matrix[:, -1])\n",
    "        # append to the best_path, the tag associated with the best_final_state that is the most probable tag for the last word of the sentence\n",
    "        best_path.append(tag_list[best_final_state])\n",
    "\n",
    "        # iterate backward through the words of the sentence from last word to first word to reconstruct the best tag sequence using the backpointers matrix\n",
    "        for w in range(len(words) - 1, 0, -1):\n",
    "            # update the best_final_state using the retrieved previous state tag that led to the current best_final_state at position w\n",
    "            best_final_state = backpointers[best_final_state][w]\n",
    "            # retrieved tag that is the previous state is then inserted at the beginning of the best_path \n",
    "            best_path.insert(0, tag_list[best_final_state])\n",
    "        # reconstructed optimal sequence of predicted tags that maximizes the probabiltiy of the entire tag sequence for the current input test sentence\n",
    "        predicted_tags.append(best_path)\n",
    "\n",
    "    return predicted_tags\n",
    "\n",
    "\n",
    "sentence_tags = []\n",
    "\n",
    "for sentence in training_data:\n",
    "    sentence_tag = \" \".join([tag for _, tag in sentence])\n",
    "    sentence_tags.append(sentence_tag)\n",
    "\n",
    "mm = MY_HMM()\n",
    "# Run the viterbi function and print the predicted tags\n",
    "predicted_tags = viterbi()\n",
    "print(predicted_tags)\n",
    "# test_data = [\n",
    "#     [\"Bill\", \"Gates\", \"founded\", \"Microsoft\"]\n",
    "# ]\n",
    "for i, sentence in enumerate(test_data):\n",
    "    print(\"Sentence {}:\".format(i + 1))\n",
    "    print(\"Words:\", sentence)\n",
    "    print(\"Predicted Tags:\", predicted_tags[i])\n",
    "    print()\n",
    "\n",
    "\n",
    "predicted_labels_step1 = predicted_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **STEP 2:**\n",
    "##### Use the NLTK ne_chunk() function for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output using NLTK ne_chunk() function for NER: \n",
      "(S\n",
      "  (PERSON Bill/NNP)\n",
      "  (PERSON Gates/NNP)\n",
      "  founded/VBD\n",
      "  (PERSON Microsoft/NNP))\n",
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION Louvre/NNP Museum/NNP)\n",
      "  is/VBZ\n",
      "  in/IN\n",
      "  (GPE Paris/NNP))\n",
      "(S\n",
      "  (PERSON Mount/NNP)\n",
      "  (ORGANIZATION Fuji/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  famous/JJ\n",
      "  landmark/NN\n",
      "  in/IN\n",
      "  (GPE Japan/NNP))\n",
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION United/NNP Nations/NNP)\n",
      "  was/VBD\n",
      "  formed/VBN\n",
      "  in/IN\n",
      "  1945/CD)\n",
      "(S\n",
      "  (PERSON Shakira/NNP)\n",
      "  performed/VBD\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Super/NNP Bowl/NNP)\n",
      "  halftime/NN\n",
      "  show/NN)\n",
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION Nobel/NNP Peace/NNP Prize/NNP)\n",
      "  was/VBD\n",
      "  awarded/VBN\n",
      "  to/TO\n",
      "  (PERSON Malala/NNP Yousafzai/NNP))\n",
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION Amazon/NNP River/NNP)\n",
      "  flows/VBZ\n",
      "  through/IN\n",
      "  (PERSON Brazil/NNP))\n",
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION Pyramids/NNP)\n",
      "  of/IN\n",
      "  (GPE Giza/NNP)\n",
      "  are/VBP\n",
      "  in/IN\n",
      "  (GPE Egypt/NNP))\n",
      "(S (GPE Rome/NNP) is/VBZ the/DT capital/NN of/IN (GPE Italy/NNP))\n",
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION Great/NNP Wall/NNP)\n",
      "  of/IN\n",
      "  (GPE China/NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Seven/NNP Wonders/NNP)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  World/NNP)\n",
      "\n",
      "\n",
      "[Tree('S', [Tree('PERSON', [('Bill', 'NNP')]), Tree('PERSON', [('Gates', 'NNP')]), ('founded', 'VBD'), Tree('PERSON', [('Microsoft', 'NNP')])]), Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Louvre', 'NNP'), ('Museum', 'NNP')]), ('is', 'VBZ'), ('in', 'IN'), Tree('GPE', [('Paris', 'NNP')])]), Tree('S', [Tree('PERSON', [('Mount', 'NNP')]), Tree('ORGANIZATION', [('Fuji', 'NNP')]), ('is', 'VBZ'), ('a', 'DT'), ('famous', 'JJ'), ('landmark', 'NN'), ('in', 'IN'), Tree('GPE', [('Japan', 'NNP')])]), Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('United', 'NNP'), ('Nations', 'NNP')]), ('was', 'VBD'), ('formed', 'VBN'), ('in', 'IN'), ('1945', 'CD')]), Tree('S', [Tree('PERSON', [('Shakira', 'NNP')]), ('performed', 'VBD'), ('at', 'IN'), ('the', 'DT'), Tree('ORGANIZATION', [('Super', 'NNP'), ('Bowl', 'NNP')]), ('halftime', 'NN'), ('show', 'NN')]), Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Nobel', 'NNP'), ('Peace', 'NNP'), ('Prize', 'NNP')]), ('was', 'VBD'), ('awarded', 'VBN'), ('to', 'TO'), Tree('PERSON', [('Malala', 'NNP'), ('Yousafzai', 'NNP')])]), Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Amazon', 'NNP'), ('River', 'NNP')]), ('flows', 'VBZ'), ('through', 'IN'), Tree('PERSON', [('Brazil', 'NNP')])]), Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Pyramids', 'NNP')]), ('of', 'IN'), Tree('GPE', [('Giza', 'NNP')]), ('are', 'VBP'), ('in', 'IN'), Tree('GPE', [('Egypt', 'NNP')])]), Tree('S', [Tree('GPE', [('Rome', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('capital', 'NN'), ('of', 'IN'), Tree('GPE', [('Italy', 'NNP')])]), Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Great', 'NNP'), ('Wall', 'NNP')]), ('of', 'IN'), Tree('GPE', [('China', 'NNP')]), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), Tree('ORGANIZATION', [('Seven', 'NNP'), ('Wonders', 'NNP')]), ('of', 'IN'), ('the', 'DT'), ('World', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "def perform_nltk_ner():\n",
    "    ner_tags = []\n",
    "    for sentence in test_data:\n",
    "        # Part-of-speech tagging using pos-tag\n",
    "        pos_tags = pos_tag(sentence) \n",
    "        # Perform named entity recognition (NER) using ne_chunk()\n",
    "        tags = ne_chunk(pos_tags)\n",
    "        ner_tags.append(tags)\n",
    "    return ner_tags\n",
    "\n",
    "print(\"Output using NLTK ne_chunk() function for NER: \")\n",
    "ner_tags = perform_nltk_ner()\n",
    "for tag in ner_tags:\n",
    "    print(tag)\n",
    "print(\"\\n\")\n",
    "print(ner_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the obtained tags in the tree to a list of tags and change the notations making it similar to the true labels I will be ocnsidering for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PERSON', 'PERSON', 'O', 'PERSON'], ['O', 'ORGANIZATION', 'ORGANIZATION', 'O', 'O', 'LOCATION'], ['PERSON', 'ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'LOCATION'], ['O', 'ORGANIZATION', 'ORGANIZATION', 'O', 'O', 'O', 'O'], ['PERSON', 'O', 'O', 'O', 'ORGANIZATION', 'ORGANIZATION', 'O', 'O'], ['O', 'ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION', 'O', 'O', 'O', 'PERSON', 'PERSON'], ['O', 'ORGANIZATION', 'ORGANIZATION', 'O', 'O', 'PERSON'], ['O', 'ORGANIZATION', 'O', 'LOCATION', 'O', 'O', 'LOCATION'], ['LOCATION', 'O', 'O', 'O', 'O', 'LOCATION'], ['O', 'ORGANIZATION', 'ORGANIZATION', 'O', 'LOCATION', 'O', 'O', 'O', 'O', 'ORGANIZATION', 'ORGANIZATION', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "# convert tags so that they are similar in format with true labels\n",
    "def ner_tags_converted(entity):\n",
    "    location_entities = {'LOCATION', 'GPE'}\n",
    "    if entity == 'PERSON':\n",
    "        return 'PERSON'\n",
    "    elif entity in location_entities:\n",
    "        return 'LOCATION'\n",
    "    elif entity == 'ORGANIZATION':\n",
    "        return 'ORGANIZATION'\n",
    "    else:\n",
    "        return 'O'\n",
    "\n",
    "# Define a function to convert a tree to predicted labels\n",
    "def tree_to_predicted_labels(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        # Extract the label of the current tree node\n",
    "        label = tree.label()\n",
    "        \n",
    "        if label in ['PERSON', 'LOCATION', 'ORGANIZATION', 'GPE']:\n",
    "            # If the label is an entity type, map it and return\n",
    "            return [ner_tags_converted(label)] * len(tree.leaves())\n",
    "        else:\n",
    "            # If the label is not an entity type, recursively process its children\n",
    "            predicted_labels = []\n",
    "            for subtree in tree:\n",
    "                predicted_labels.extend(tree_to_predicted_labels(subtree))\n",
    "            return predicted_labels\n",
    "    else:\n",
    "        # If the input is not a tree, return a list of 'O' labels\n",
    "        return ['O']\n",
    "\n",
    "# Convert each tree to predicted labels\n",
    "predicted_labels_step2 = [tree_to_predicted_labels(tree) for tree in ner_tags]\n",
    "print(predicted_labels_step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **STEP 3:**\n",
    "##### Use the  spaCy nlp() function for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output using spacy nlp() function for NER: \n",
      "[('Bill Gates', 'PERSON'), ('Microsoft', 'ORG')]\n",
      "[('The Louvre Museum', 'ORG'), ('Paris', 'GPE')]\n",
      "[('Mount Fuji', 'LOC'), ('Japan', 'GPE')]\n",
      "[('The United Nations', 'ORG'), ('1945', 'DATE')]\n",
      "[('Shakira', 'PERSON'), ('the Super Bowl', 'EVENT')]\n",
      "[('The Nobel Peace Prize', 'WORK_OF_ART'), ('Malala Yousafzai', 'PERSON')]\n",
      "[('Amazon River', 'LOC'), ('Brazil', 'GPE')]\n",
      "[('Giza', 'PERSON'), ('Egypt', 'GPE')]\n",
      "[('Rome', 'GPE'), ('Italy', 'GPE')]\n",
      "[('The Great Wall of China', 'FAC'), ('one', 'CARDINAL'), ('Seven', 'CARDINAL')]\n"
     ]
    }
   ],
   "source": [
    "def process_sentence():\n",
    "    for sentence in test_data:\n",
    "        text = \" \".join(sentence)\n",
    "        # Process the text with spaCy\n",
    "        doc_spacy = nlp_spacy(text)\n",
    "        # Extract named entities with their labels\n",
    "        entities_spacy = [(ent.text, ent.label_) for ent in doc_spacy.ents]\n",
    "        print(entities_spacy)\n",
    "\n",
    "print(\"Output using spacy nlp() function for NER: \")\n",
    "process_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making lists of labels for each of the steps as each modules and functions used have different notations for tags\n",
    "# Converted tags : Step 3 (spaCy (nlp()))\n",
    "# PERSON and DATE does not need to be converted\n",
    "# convert LOC,FAC,GPE => LOCATION, convert ORG => ORGANIZATION,  convert WORK_OF_ART, CARDINAL => TITLE\n",
    "\n",
    "predicted_labels_step3 = [\n",
    "    ['PERSON', 'PERSON', 'O', 'ORGANIZATION'],\n",
    "    ['ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION', 'O', 'O', 'LOCATION'],\n",
    "    ['LOCATION', 'LOCATION', 'O', 'O', 'O', 'O', 'O', 'LOCATION'],\n",
    "    ['ORGANIZATION','ORGANIZATION','ORGANIZATION','O','O','O','DATE'],\n",
    "    ['PERSON', 'O', 'O','EVENT','EVENT','EVENT','O','O'],\n",
    "    ['TITLE','TITLE','TITLE','TITLE','O','O','O','PERSON','PERSON'],\n",
    "    ['O','LOCATION','LOCATION','O','O','LOCATION'],\n",
    "    ['O','O','O','PERSON','O','O','LOCATION'],\n",
    "    ['LOCATION', 'O', 'O', 'O', 'O', 'LOCATION'],\n",
    "    ['LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'O', 'TITLE', 'O', 'O', 'TITLE', 'TITLE', 'O', 'O', 'TITLE']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **STEP 4:**\n",
    "##### Use the Huggingface/Transformers’ ner pipeline function for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output using Huggingface/Transformers’ ner pipeline function for NER: \n",
      "Test sentence: 1\n",
      "Bill Gates founded Microsoft\n",
      "Entities:  [{'entity_group': 'PER', 'score': 0.9970532, 'word': 'Bill Gates', 'start': 0, 'end': 10}, {'entity_group': 'ORG', 'score': 0.99925035, 'word': 'Microsoft', 'start': 19, 'end': 28}]\n",
      "Word: Bill Gates, Entity: PER, Score: 1.00\n",
      "Word: Microsoft, Entity: ORG, Score: 1.00\n",
      "\n",
      "\n",
      "Test sentence: 2\n",
      "The Louvre Museum is in Paris\n",
      "Entities:  [{'entity_group': 'ORG', 'score': 0.86077875, 'word': 'Louvre', 'start': 4, 'end': 10}, {'entity_group': 'LOC', 'score': 0.47728878, 'word': 'Museum', 'start': 11, 'end': 17}, {'entity_group': 'LOC', 'score': 0.99909127, 'word': 'Paris', 'start': 24, 'end': 29}]\n",
      "Word: Louvre, Entity: ORG, Score: 0.86\n",
      "Word: Museum, Entity: LOC, Score: 0.48\n",
      "Word: Paris, Entity: LOC, Score: 1.00\n",
      "\n",
      "\n",
      "Test sentence: 3\n",
      "Mount Fuji is a famous landmark in Japan\n",
      "Entities:  [{'entity_group': 'LOC', 'score': 0.5602856, 'word': 'Mount', 'start': 0, 'end': 5}, {'entity_group': 'PER', 'score': 0.48292866, 'word': 'Fuji', 'start': 6, 'end': 10}, {'entity_group': 'LOC', 'score': 0.9984364, 'word': 'Japan', 'start': 35, 'end': 40}]\n",
      "Word: Mount, Entity: LOC, Score: 0.56\n",
      "Word: Fuji, Entity: PER, Score: 0.48\n",
      "Word: Japan, Entity: LOC, Score: 1.00\n",
      "\n",
      "\n",
      "Test sentence: 4\n",
      "The United Nations was formed in 1945\n",
      "Entities:  [{'entity_group': 'ORG', 'score': 0.9989638, 'word': 'United Nations', 'start': 4, 'end': 18}]\n",
      "Word: United Nations, Entity: ORG, Score: 1.00\n",
      "\n",
      "\n",
      "Test sentence: 5\n",
      "Shakira performed at the Super Bowl halftime show\n",
      "Entities:  [{'entity_group': 'PER', 'score': 0.9666548, 'word': 'Shakira', 'start': 0, 'end': 7}, {'entity_group': 'MISC', 'score': 0.98595226, 'word': 'Super Bowl', 'start': 25, 'end': 35}]\n",
      "Word: Shakira, Entity: PER, Score: 0.97\n",
      "Word: Super Bowl, Entity: MISC, Score: 0.99\n",
      "\n",
      "\n",
      "Test sentence: 6\n",
      "The Nobel Peace Prize was awarded to Malala Yousafzai\n",
      "Entities:  [{'entity_group': 'MISC', 'score': 0.9957908, 'word': 'Nobel Peace Prize', 'start': 4, 'end': 21}, {'entity_group': 'PER', 'score': 0.9272184, 'word': 'Malala Yousafzai', 'start': 37, 'end': 53}]\n",
      "Word: Nobel Peace Prize, Entity: MISC, Score: 1.00\n",
      "Word: Malala Yousafzai, Entity: PER, Score: 0.93\n",
      "\n",
      "\n",
      "Test sentence: 7\n",
      "The Amazon River flows through Brazil\n",
      "Entities:  [{'entity_group': 'LOC', 'score': 0.78579485, 'word': 'Amazon River', 'start': 4, 'end': 16}, {'entity_group': 'LOC', 'score': 0.9993777, 'word': 'Brazil', 'start': 31, 'end': 37}]\n",
      "Word: Amazon River, Entity: LOC, Score: 0.79\n",
      "Word: Brazil, Entity: LOC, Score: 1.00\n",
      "\n",
      "\n",
      "Test sentence: 8\n",
      "The Pyramids of Giza are in Egypt\n",
      "Entities:  [{'entity_group': 'LOC', 'score': 0.8445041, 'word': 'Pyramid', 'start': 4, 'end': 11}, {'entity_group': 'LOC', 'score': 0.9356552, 'word': 'of Giza', 'start': 13, 'end': 20}, {'entity_group': 'LOC', 'score': 0.9997415, 'word': 'Egypt', 'start': 28, 'end': 33}]\n",
      "Word: Pyramid, Entity: LOC, Score: 0.84\n",
      "Word: of Giza, Entity: LOC, Score: 0.94\n",
      "Word: Egypt, Entity: LOC, Score: 1.00\n",
      "\n",
      "\n",
      "Test sentence: 9\n",
      "Rome is the capital of Italy\n",
      "Entities:  [{'entity_group': 'LOC', 'score': 0.9991357, 'word': 'Rome', 'start': 0, 'end': 4}, {'entity_group': 'LOC', 'score': 0.99889237, 'word': 'Italy', 'start': 23, 'end': 28}]\n",
      "Word: Rome, Entity: LOC, Score: 1.00\n",
      "Word: Italy, Entity: LOC, Score: 1.00\n",
      "\n",
      "\n",
      "Test sentence: 10\n",
      "The Great Wall of China is one of the Seven Wonders of the World\n",
      "Entities:  [{'entity_group': 'LOC', 'score': 0.8930453, 'word': 'Great Wall of China', 'start': 4, 'end': 23}, {'entity_group': 'MISC', 'score': 0.9910465, 'word': 'Seven Wonders of the World', 'start': 38, 'end': 64}]\n",
      "Word: Great Wall of China, Entity: LOC, Score: 0.89\n",
      "Word: Seven Wonders of the World, Entity: MISC, Score: 0.99\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [' '.join(words) for words in test_data]\n",
    "def ner_pipeline():\n",
    "    # Load NER pipeline with the specified model\n",
    "    ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)\n",
    "    print(\"Output using Huggingface/Transformers’ ner pipeline function for NER: \")\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        entities_dict = ner(sentence)\n",
    "        print(f\"Test sentence: {i+1}\")\n",
    "        print(sentence)\n",
    "        print(\"Entities: \", entities_dict)\n",
    "        for entity in entities_dict:\n",
    "            print(f\"Word: {entity['word']}, Entity: {entity['entity_group']}, Score: {entity['score']:.2f}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "ner_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making lists of labels for each of the steps as each modules and functions used have different notations for tags\n",
    "# Converted tags : Step 4 (HuggingFace/Transformers) \n",
    "# Convert PER => PERSON, Convert LOC => LOCATION, Convert ORG => ORGANIZATION, Convert MISC => TITLE\n",
    "\n",
    "predicted_labels_step4 = [['PERSON', 'PERSON', 'O', 'ORGANIZATION'],\n",
    "    ['O', 'ORGANIZATION', 'LOCATION', 'O', 'O', 'LOCATION'],\n",
    "    ['LOCATION', 'PERSON', 'O', 'O', 'O', 'O', 'O', 'LOCATION'],\n",
    "    ['O', 'ORGANIZATION', 'ORGANIZATION', 'O', 'O', 'O', 'O'],\n",
    "    ['PERSON', 'O', 'O', 'O', 'TITLE', 'TITLE', 'O', 'O'],\n",
    "    ['O', 'TITLE', 'TITLE', 'TITLE', 'O', 'O', 'O', 'PERSON', 'PERSON'],\n",
    "    ['O', 'LOCATION', 'LOCATION', 'O', 'O', 'LOCATION'],\n",
    "    ['O', 'LOCATION', 'LOCATION', 'LOCATION', 'O', 'O', 'LOCATION'],\n",
    "    ['LOCATION', 'O', 'O', 'O', 'O', 'LOCATION'],\n",
    "    ['O', 'LOCATION', 'LOCATION', 'LOCATION', 'LOCATION', 'O', 'O', 'O', 'O', 'TITLE', 'TITLE', 'TITLE', 'TITLE', 'TITLE']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **STEP 5:** \n",
    "##### Use the stanza’s ner pipeline for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bill Gates', 'PERSON'), ('Microsoft', 'ORG')]\n",
      "[('The Louvre Museum', 'FAC'), ('Paris', 'GPE')]\n",
      "[('Mount Fuji', 'LOC'), ('Japan', 'GPE')]\n",
      "[('The United Nations', 'ORG'), ('1945', 'DATE')]\n",
      "[('Shakira', 'PERSON'), ('the Super Bowl', 'EVENT')]\n",
      "[('The Nobel Peace Prize', 'WORK_OF_ART'), ('Malala Yousafzai', 'PERSON')]\n",
      "[('The Amazon River', 'LOC'), ('Brazil', 'GPE')]\n",
      "[('The Pyramids of Giza', 'PERSON'), ('Egypt', 'GPE')]\n",
      "[('Rome', 'GPE'), ('Italy', 'GPE')]\n",
      "[('China', 'GPE'), ('Seven', 'CARDINAL')]\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(sentences):\n",
    "    extracted_entities = []\n",
    "    for sentence in sentences:\n",
    "        # Join words to form a sentence\n",
    "        text = ' '.join(sentence)\n",
    "        # Process the text with the NER pipeline\n",
    "        doc = nlp_stanza(text)\n",
    "        entities = []\n",
    "        for ent in doc.ents:\n",
    "            # Append entity text and its type\n",
    "            entities.append((ent.text, ent.type))\n",
    "        extracted_entities.append(entities)\n",
    "    for entities in extracted_entities:\n",
    "        print(entities)\n",
    "\n",
    "extract_entities(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making lists of labels for each of the steps as each modules and functions used have different notations for tags\n",
    "# Converted tags : Step 5 (stanza) \n",
    "# PERSON, DATE does not need to be converted\n",
    "# Convert LOC,FAC,GPE => LOCATION, convert ORG => ORGANIZATION and convert WORK_OF_ART, CARDINAL => TITLE\n",
    "\n",
    "predicted_labels_step5= [\n",
    "    ['PERSON', 'PERSON', 'O', 'ORGANIZATION'],\n",
    "    ['ORGANIZATION', 'ORGANIZATION', 'ORGANIZATION', 'O', 'O', 'LOCATION'],\n",
    "    ['LOCATION', 'LOCATION', 'O', 'O', 'O', 'O', 'O', 'LOCATION'],\n",
    "    ['ORGANIZATION','ORGANIZATION','ORGANIZATION','O','O','O','DATE'],\n",
    "    ['PERSON', 'O', 'O','EVENT','EVENT','EVENT','O','O'],\n",
    "    ['TITLE','TITLE','TITLE','TITLE','O','O','O','PERSON','PERSON'],\n",
    "    ['LOCATION','LOCATION','LOCATION','O','O','LOCATION'],\n",
    "    ['PERSON','PERSON','PERSON','PERSON','O','O','LOCATION'],\n",
    "    ['LOCATION', 'O', 'O', 'O', 'O', 'LOCATION'],\n",
    "    ['O', 'O', 'O', 'O', 'LOCATION', 'O', 'O', 'O', 'O', 'TITLE', 'O', 'O', 'O', 'O']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **STEP 6:**\n",
    "##### Evaluate the performance of all the above steps in terms of accuracy, precision and recall, either as a whole (micro-averaging), or category by category (macro-averaging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true label for evaluation of all the above steps 1 to 5\n",
    "true_labels = [\n",
    "    ['PERSON', 'PERSON', 'O', 'ORGANIZATION'],\n",
    "    ['O', 'LOCATION', 'LOCATION', 'O', 'O', 'LOCATION'],\n",
    "    ['LOCATION', 'LOCATION', 'O', 'O', 'O', 'O', 'O', 'LOCATION'],\n",
    "    ['LOCATION','LOCATION','LOCATION','O','O','O','DATE'],\n",
    "    ['PERSON', 'O', 'O','O','EVENT','EVENT','O','O'],\n",
    "    ['O','TITLE','TITLE','TITLE','O','O','O','PERSON','PERSON'],\n",
    "    ['O','LOCATION','LOCATION','O','O','LOCATION'],\n",
    "    ['O','LOCATION','LOCATION','LOCATION','O','O','LOCATION'],\n",
    "    ['LOCATION', 'O', 'O', 'TITLE', 'O', 'LOCATION'],\n",
    "    ['O', 'LOCATION', 'LOCATION', 'O', 'LOCATION', 'O', 'TITLE', 'O', 'O', 'TITLE', 'TITLE', 'O', 'O', 'TITLE']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_combined = []\n",
    "\n",
    "# List of predicted labels from different steps\n",
    "prediction_lists_all_steps = [predicted_labels_step1, predicted_labels_step2, predicted_labels_step3, predicted_labels_step4, predicted_labels_step5]\n",
    "\n",
    "# Extend predictions_combined with all lists in predicted_lists\n",
    "predictions_combined.extend(prediction_lists_all_steps)\n",
    "\n",
    "# evaluating performance using micro averaging\n",
    "def micro_averaging(true_labels, pred_labels):\n",
    "    # calculating total number of labels for all the sentences in the test data\n",
    "    total_labels = sum(len(true) for true in true_labels)\n",
    "    # Count the number of correct predictions (true_preds) where the true label matches predicted label\n",
    "    true_preds = sum(sum(1 for t, p in zip(true, pred) if t == p) for true, pred in zip(true_labels, pred_labels))\n",
    "    # Count the number of true positive predictions (true_positive_preds) excluding 'O' labels\n",
    "    true_positive_preds = sum(sum(1 for t, p in zip(true, pred) if t == p != 'O') for true, pred in zip(true_labels, pred_labels))\n",
    "    # Count the number of predicted positive labels (pred_positives) excluding 'O' labels\n",
    "    pred_positives = sum(sum(1 for p in pred if p != 'O') for pred in pred_labels)\n",
    "    # Count the number of actual positive labels (actual_positives) excluding 'O' labels\n",
    "    actual_positives = sum(1 for true in true_labels for t in true if t != 'O')\n",
    "\n",
    "    # calculating the accuracy - ratio of correct predictions(true_preds) to the total number of true labels(total_labels)\n",
    "    accuracy = true_preds / total_labels\n",
    "    # calculating precision - ratio of true positive predictions(true_positive_preds) to predicted positive labels(pred_positives),\n",
    "    precision = true_positive_preds / pred_positives if pred_positives > 0 else 0 # handle cases where there are no predicted positive labels\n",
    "    # calculating recall - ratio of true positive predictions(true_positive_preds) to actual positive labels(actual_positives),\n",
    "    recall = true_positive_preds / actual_positives if actual_positives > 0 else 0 # handle cases where there are no actual positive labels\n",
    "    \n",
    "    return accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Printing the Micro Averaging Accuracy, Micro Averaging Precision and Micro Averaging Recall for the following steps:**\n",
    "###### STEP 1: Code from scratch to implement the Viterbi algorithm and a Hidden Markov Model\n",
    "###### STEP 2 : Use the NLTK ne_chunk() function for NER\n",
    "###### STEP 3 : Use the  spaCy nlp() function for NER\n",
    "###### STEP 4 : Use the Huggingface/Transformers’ ner pipeline function for NER\n",
    "###### STEP 5: Use the stanza’s ner pipeline for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction for Step 1 : \n",
      "Micro-average Accuracy: 0.4666666666666667\n",
      "Micro-average Precision: 0.30612244897959184\n",
      "Micro-average Recall: 0.39473684210526316\n",
      "\n",
      "Prediction for Step 2 : \n",
      "Micro-average Accuracy: 0.6533333333333333\n",
      "Micro-average Precision: 0.375\n",
      "Micro-average Recall: 0.3157894736842105\n",
      "\n",
      "Prediction for Step 3 : \n",
      "Micro-average Accuracy: 0.8133333333333334\n",
      "Micro-average Precision: 0.725\n",
      "Micro-average Recall: 0.7631578947368421\n",
      "\n",
      "Prediction for Step 4 : \n",
      "Micro-average Accuracy: 0.8266666666666667\n",
      "Micro-average Precision: 0.7567567567567568\n",
      "Micro-average Recall: 0.7368421052631579\n",
      "\n",
      "Prediction for Step 5 : \n",
      "Micro-average Accuracy: 0.7466666666666667\n",
      "Micro-average Precision: 0.6486486486486487\n",
      "Micro-average Recall: 0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "for step_num, prediction_label in enumerate(predictions_combined, start=1):\n",
    "    print(f\"\\nPrediction for Step {step_num} : \")\n",
    "    micro_avg_accuracy, micro_avg_precision, micro_avg_recall = micro_averaging(true_labels, prediction_label)\n",
    "    print(\"Micro-average Accuracy:\", micro_avg_accuracy)\n",
    "    print(\"Micro-average Precision:\", micro_avg_precision)\n",
    "    print(\"Micro-average Recall:\", micro_avg_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
